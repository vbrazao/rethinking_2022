---
title: "Homework - Week 4"
author: "Vasco Braz√£o"
output:
  html_document:
    keep_md: yes
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message = FALSE, warning = FALSE}
library(rethinking)
library(tidyverse)

set.seed(29012022)
```

## 1

```{r one.data}
d <- sim_happiness(seed = 1977, N_years = 1000)

d2 <- d[d$age > 17, ]

d2$A <- (d2$age-18) / (65 -18)

d2$mid <- d2$married + 1
```

```{r one.models, cache = TRUE}
m6.9 <- ulam(
  alist(
    happiness ~ dnorm(mu, sigma),
    mu <- a[mid] + bA*A,
    a[mid] ~ dnorm(0, 1),
    bA ~ dnorm(0, 2),
    sigma ~ dexp(1)
  ), data = d2, chains = 4, cores = 4, log_lik = TRUE
)

m6.10 <- ulam(
  alist(
    happiness ~ dnorm(mu, sigma),
    mu <- a + bA*A,
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 2),
    sigma ~ dexp(1)
  ), data = d2, chains = 4, cores = 4, log_lik = TRUE
)


# m6.9 <- quap(
#   alist(
#     happiness ~ dnorm(mu, sigma),
#     mu <- a[mid] + bA*A,
#     a[mid] ~ dnorm(0, 1),
#     bA ~ dnorm(0, 2),
#     sigma ~ dexp(1)
#   ), data = d2
# )
# 
# m6.10 <- quap(
#   alist(
#     happiness ~ dnorm(mu, sigma),
#     mu <- a + bA*A,
#     a ~ dnorm(0, 1),
#     bA ~ dnorm(0, 2),
#     sigma ~ dexp(1)
#   ), data = d2
# )
```

```{r one.model.precis}
precis(m6.9, depth = 2)

precis(m6.10)
```

```{r one.model.comparison}
compare(m6.9, m6.10)

compare(m6.9, m6.10, func = PSIS)
```

Both indicators prefer model m6.9, which predicts happiness by age while stratifying for marriage status.

Assuming the simple DAG in the book, $H \rightarrow M \leftarrow A$, we are capturing a non-causal association between age and happiness, brought about by conditioning on collider M. Still, we can interpret the coefficients as follows: Once we know someone's marital status, knowing their age helps us to predict their happiness more accurately. Symmetrically, if we already know someone's age, learning their marital status helps us to better predict their happiness.

## 2

```{r two.data}
data("foxes")

d2 <- foxes %>% 
  dplyr::mutate(
    A = rethinking::standardize(area),
    F = rethinking::standardize(avgfood),
    W = rethinking::standardize(weight),
    G = rethinking::standardize(groupsize)
  )
```

To solve this exercise, we first run the same models as last week, and include one with just the interaction of food and group size for fun.

```{r two.models}
# weight on food
m1 <- rethinking::quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + b*F,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), 
  data = d2
)

# weight on food and groupsize
m2 <- rethinking::quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bF*F + bG*G,
    a ~ dnorm(0, 0.2),
    bF ~ dnorm(0, 1),
    bG ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), 
  data = d2
)

# weight on interaction food by groupsize
m3 <- rethinking::quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bFG*F*G,
    a ~ dnorm(0, 0.2),
    bFG ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), 
  data = d2
)

```

```{r two.models.precis}
precis(m1)

precis(m2)

precis(m3)
```

```{r two.models.compare}
compare(m1, m2, m3)

compare(m1, m2, m3, func = PSIS)
```

Both time, m2 wins - but barely. In that model, bF is the direct effect of food on weight, while bG is the total effect of groupsize on weight (because we controlled for confounder F).

# 3

```{r three.data}
data("cherry_blossoms")

d3.0 <- cherry_blossoms %>% 
  filter(
    complete.cases(doy, temp)
  ) 

d3 <- d3.0 %>%  
  mutate(
    across(
      .cols = -year,
      .fns = ~rethinking::standardize(.x)
    )
  ) 
```

```{r three.models}
m3.1 <- rethinking::quap(
  alist(
    doy ~ dnorm(mu, sigma),
    mu <- a + b*temp,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), data = d3
)

m3.2 <- rethinking::quap(
  alist(
    doy ~ dstudent(2,mu,sigma),
    mu <- a + b*temp,
    a ~ dnorm(0, 0.2),
    b ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), data = d3
)

m3.3 <- rethinking::quap(
  alist(
    doy ~ dnorm(mu,sigma),
    mu <- a + b*temp,
    sigma <- as + b2*temp,
    a ~ dnorm(0, 0.2),
    as ~ dexp(1),
    b ~ dnorm(0, 1),
    b2 ~ dnorm(0, 0.2)
  ), data = d3
)
```

```{r three.compare}
compare(m3.1, m3.2, m3.3)

compare(m3.1, m3.2, m3.3, func = PSIS)
```

The thirds model wins both times. In this model, we modeled the mu of doy as a linear function of temperature, and we also modeled the sigma as a linear function of temperature, which seems to provide an edge in out of sample prediction. So we will use samples from this posterior to predict the doy if march temperature reached 9 degrees. 

First, we need to turn 9 degrees into a z-score.

```{r three.zscoring}
meantemp <- mean(d3.0$temp)
meandoy <- mean(d3.0$doy)

sdtemp <- sd(d3.0$temp)
sddoy <- sd(d3.0$doy)

z9 <- (9-meantemp)/sdtemp
```

```{r three.predict}
simdoy <- rethinking::sim(m3.3, data = list(temp = z9))

# unstandardize please

pred.doy <- simdoy*sddoy + meandoy

dens(pred.doy, col = 3)

#to compare with observed data

dens(d3.0$doy, add = TRUE)
```

We see that our model predicts both a much earlier blossom as well as a tighter density (because it expects SD to shrink as temperatures rise).

Cool! My first time modeling SD :D



